\section{Surrogate modeling}
Meta-PDE is a surrogate modeling approach.
Surrogate models are useful when one will need to evaluate the solution of many
PDEs from a similar family and is willing to pay the up-front cost of training a
surrogate in order to solve the downstream probelms with minimal cost-per-PDE.
The most popular application is in design or system identification, where a PDE
must be solved at each step optimizing the PDE's parameters, boundary conditions, or geometry,
and it is convenient to replace the PDE solver with a cheap surrogate~\citep{kochenderfer2019algorithms}.
The optimization may be numerical ("PDE-constrained optimization") or by hand.
If numerical, gradients can be obtained using the adjoint method with cost equivalent
to one solve of a linearized version of the PDE.
In both cases, therefore, the optimization bottleneck is in the "forward pass" of solving the PDE.
Surrogate models are also particularly relevant in scenarios involving dynamics models
(where a PDE must be solved at each timestep),
and where real-time analysis is required for the convenience of a human designer or
to allow embedding the PDE solver in a control policy.

There are many approaches to surrogate modeling, such as
random forests~\citep{criminisi2011decision},
Gaussian processes~\citep{shahriari2015taking},
Student-$t$ processes~\citep{shah2014student},
and neural networks~\citep{snoek2015scalable}.
Regression can be from the PDE's parameters to coefficients of the PDE solution,
yielding a general-purpose surrogate for that distribution of PDEs, or from
PDE parameters to an objective value for a specific optimization problem.
Most surrogate modeling procedures have two key restrictions:
\begin{enumerate}
  \item They require supervised training data in the form of
  (PDE parameter, PDE solution) or (PDE parameter, objective value) pairs,
  which must be generated by an expensive ground-truth PDE solver, \label{itm:supervision}
  \item They regress from a vector representation of the PDE or its parameters to a
  vector representaion of the solution; having to fix these vector bases
  makes it difficult to fit surrogates for
  distributions of PDEs containin highly varied structure in geometry or governing
  equations.\label{itm:structure}
\end{enumerate}
Restriction \ref{itm:supervision} means that in order to be of net computational benefit,
the surrogate must be applied to many more downstream problems than the number of
data points required to fit a good model. For rich classes of PDEs and expressive models,
it can take tens or hundreds of thousands of data points to fit a model which can generalize,
so this greatly limits surrogate models' use case.

Restriction \ref{itm:structure} is not an issue if the problem can be phrased as
regressing from coefficients of a parameter or boundary condition in a piecewise polynomial
basis to coefficients of a solution in a similar basis, with the discretization
fixed across problems.
However, many problems are not amenable to such framing.
Problems with different domain geometry or very different parameters and solutions
may require very different discretizations to represent them well;
generating a good mesh for a given problem can itself be a hard problem and can be a
greater bottleneck than the cost of the base PDE solve.
The factors of variation of a PDE (the domain, governing equations, and parameters)
are naturally and generally expressed as functions on the domain and as
operators on solutions: requiring them to be parameterized by vectors restricts
the classes of PDEs to which surrogate models can be applied and greatly limits their
use case.

Several recent works have relaxed one of these restrictions.
\citet{zhu2019physics} amortize the finite differentce method,
predicting PDE solutions from parameter fields with a
ConvNet by training the ConvNet to produce solutions with minimum variational energy
across a distribution of problems.
This removes restriction \ref{itm:supervision} but requiring the discretization
to be a fixed uniform grid.
\citet{xue2020amortized} amortize the finite element method, training a surrogate model
to minimize a similar variational energy but use a finite element discretization:
their model can handle complicated geometry, and by measuring the variational energy
in the finite element basis maintains some desirable properties of finite element
analysis. This avoids restriction \ref{itm:supervision} and allows arbitrary
meshes but requires a single mesh be fixed for all PDEs in the distribution to be
amortized.
Graph Neural Network based approaches \citep{sanchez2020learning,pfaff2020learning},
which learn a forward operator in terms of
interactions between nearby particles or mesh cells,
allow for arbitrary geometries (partially removing restriction
\ref{itm:structure})
but require supervised data and have not yet produced surrogate models which are
cheaper to evaluate than ground-truth solvers when tested on equivalent hardware.
Neural Operator based approaches \citep{li2020neural,li2020fourier} learn a map from
initial conditions or parameters to solution which builds in
some invariance to mesh \emph{resolution}, but require the mesh to be a uniform
grid, and also require expensive supervised data.
