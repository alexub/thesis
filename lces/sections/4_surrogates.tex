\section{Composable energy surrogates}
\label{sec:surrogates}
We apply the idea of learning collapsed objectives to the problem of simulating two-dimensional cellular mechanical meta-material behavior.
The material response is determined by the displacement field $u$ which minimizes the energy~${\int_{\Omega} W dX}$, subject to boundary conditions.
We divide $\Omega$ into regular square subregions $\Omega_i$, which we choose to be cells with $2\times2$ arrays of pores, and denote the intersection of the subregion boundaries with~${\mathcal{B} = \partial\Omega_1 \cup \partial\Omega_2 \cup} \dots$
We let $u_i$ be the restriction of $u$ to $\Omega_i$.
We take the quantity of interest to be $u_{\mathcal{B}}$, the restriction of $u$ to $\mathcal{B}$, and the nuisance variables to be the restriction of $u$ to~${\Omega}$\textbackslash~${\mathcal{B}}$.
The partitioning of $\Omega$ is shown in Figure \ref{Fig:decomp}.

The total energy decomposes as a sum over regions:
\begin{align*}
    E(u) = \int_{X \in \Omega} W(u) dX \quad \quad = \sum_i \int_{X \in \Omega_i} W(u_i) dX := \sum_i E(u_i)
\end{align*}\vspace{-0.1cm}
Let $\tilde{u}_i$ be the restriction of $u$ to $\partial\Omega_i$. Note~${\partial\Omega_i = \mathcal{B} \cap \Omega_i}$.
Let the collapsed component energy be:
\begin{align*}
    \tilde{E}_i (\tilde{u}_i) := \min_{u_i} E (u_i) \quad \text{subject to } u_i(X) = \tilde{u}_i(X)\quad X \in \partial{\Omega_i}\,.
\end{align*}
This quantity is the lowest energy achievable by displacements of the \emph{interior} of the cell~$\Omega_i$, given the boundary conditions specified by~$\tilde{u}_i$ on~$\partial{\Omega_i}$.
$\tilde{E}_i(\tilde{u}_i)$ depends on the shape of the region $\Omega_i$, i.e., on the geometry of the pores.
Rather than each possible pore shape having a unique collapsed energy function, we introduce the pore shape parameter~${\xi = (\alpha, \beta)}$ as an argument, replacing~$\tilde{E}_i(\tilde{u_i})$ with~$\tilde{E}(\tilde{u}_i, \xi_i)$.
The macroscopic behavior of the material is fully determined by this \emph{single} collapsed energy function~${\tilde{E}(\tilde{u}_i, \xi_i)}$.
Given the true collapsed energy functions, we could accurately simulate material behavior in the reduced basis of the boundaries between each component~$\Omega_i$.\footnote{So long as forces and constraints are only applied on $\mathcal{B}$.}

We learn to approximate this collapsed energy function from data. This function may be duplicated and composed to simulate the material in the reduced basis~$\mathcal{B}$, an approach we term \emph{composable energy surrogates} (CESs).
A single CES is trained to approximate the function $\tilde{E}$ by fitting to supervised data~${(\tilde{u}_i, \xi_i, \tilde{E}(\tilde{u}_i, \xi_i))}$, where $\xi_i$ and $\tilde{u}_i$ may be drawn from any distribution corresponding to anticipated pore shapes and displacements, and the targets $\tilde{E}(\tilde{u}_i, \xi_i)$ are generated by solving the PDE in a small region $\Omega_i$ with geometry defined by $\xi_i$ and with $\tilde{u}_i$ imposed as a boundary condition.
This CES may be used to approximate the energy in multiple spatial locations: it may be "composed" to approximate the total energy of larger cellular meta-materials.

To efficiently solve for a reduced-basis displacement~$u_{\mathcal{B}}$ on~$\mathcal{B}$,
we minimize the composed surrogate energy,~${\hat{E}(u_\mathcal{B}) = \sum_i \hat{E}(\tilde{u}_i, \xi_i)}$, where~$\hat{E}(\tilde{u}_i, \xi_i)$ is the model's
prediction of $\tilde{E}(\tilde{u}_i, \xi_i)$, the collapsed energy of one component.
Training CES which produce accurate reduced-basis solutions may
be thought of as a highly-structured imitation learning problem.
A sufficient condition for finding the correct minimum is for the "action" taken by the surrogate---the derivative of the energy approximation~${\nabla_{u_{\mathcal{B}}} \hat{E}}$---to match the "action" taken by an expert---the \emph{total} derivative,~${\nabla_{u_{\mathcal{B}}} \min_{u\notin \mathcal{B}} E(u)}$---along the optimization trajectory.
If so, the surrogate will follow the trajectory of a valid, if non-standard, bilevel gradient-based procedure for minimizing the energy, corresponding to (\ref{eq:bilevel}). Given an imperfect surrogate, the error in the final solution will depend on
the error in approximating~${\nabla_{u_{\mathcal{B}}} \min_{u\notin \mathcal{B}} E(u)}$
with $\nabla_{u_{\mathcal{B}}} \hat{E}$ along the trajectory.
This observation informs our model, training, and data collection procedures, described in the following sections.
%\setlength{\abovedisplayskip}{6pt}
%\setlength{\belowdisplayskip}{5pt}
\section{Model architecture}
\label{sec:model}
Our CESs take the form of a neural architecture, designed to respect known properties of the true potential energy and to maximize usefulness as surrogate energy to be minimized via a gradient-based procedure.
The effects of these design choices are quantified via an ablation study in the appendix.

\textbf{Reduced-basis parameterization}.
We use one cubic spline for each horizontal and vertical displacement function along each face of the square, with evenly spaced control points and ``not-a-knot'' boundary conditions.
Our vector representation of $\tilde{u}$ is~${\rvu \in \mathbb{R}^{2n}}$, formed from the horizontal and the vertical displacement values at each of the $n$ control points.
Splines on adjacent faces share a control point at the corner. Using $N$ control points to parameterize the function along each face requires~${n = 4*(N-1)}$ control points to parameterize a $1d$ function around a single cell.
For all experiments we use~${N = 10}$ control points along each edge, resulting in~${\rvu \in \mathbb{R}^{72}}$.

\textbf{Model structure and loss}. Our model structure and losses are shown below.
In the energy model $\hat{E}$,
$f_\phi$ is a neural network with parameters $\phi$ and $\mathcal{R}$ removes rigid-body rotation and translation.
Our loss function is~${\mathcal{L} =  \mathcal{L}^0 + \mathcal{L}^1 + \mathcal{L}^2\,,}$
which is a weighted sum of losses on the $0$th, $1$st and $2$nd energy derivatives.
$\nabla_{\rvu}$ and $\nabla^2_{\rvu}$ are the gradient and Hessian of the surrogate energy $\hat{E}$ or the ground-truth energy $\tilde{E}$ with respect to $\rvu$, and $v$ is sampled independently for each training example in a batch.
\small
\begin{align*}
    \hat{E}(\rvu, \xi) = \underbrace{||\mathcal{R}(\rvu)||_2^2}_\text{Linear elastic component} \underbrace{\exp \{f_\phi \big(\mathcal{R}(\rvu), \xi\big)\}}_\text{Stiffness}\,,\quad \quad
    \mathcal{L}^0 = \underbrace{\left\lVert f_\phi \big(\mathcal{R}(\rvu), \xi\big) - \log \frac{\tilde{E}(\tilde{u})}{||\mathcal{R}(\rvu)||_2^2} \right\rVert_2^2}_\text{Log-stiffness loss}\,, \\
    \mathcal{L}^1 = \underbrace{1 - \frac{\langle \nabla_{\rvu} \hat{E}, \nabla_{\rvu} \tilde{E}\rangle}{||\nabla_{\rvu} \hat{E}|| ||\nabla_{\rvu} \tilde{E}||}}_\text{Cosine distance between gradients}\,,\quad \quad
    \mathcal{L}^2 = \underbrace{1 - \frac{\langle \nabla^2_{\rvu} \hat{E} v,  \nabla^2_{\rvu} \tilde{E} v \rangle}{||\nabla^2_{\rvu} \hat{E} v|| ||\nabla^2_{\rvu} \tilde{E} v||}}_{\substack{\text{Cosine distance between}\\\text{Hessian-vector products}}} \quad \underbrace{v \sim \mathcal{N}(0, I^{2n})}_\text{Projection vector for Hessian}\,.
\end{align*}\normalsize
\textbf{Invariance to rigid body transforms}.
The true elastic energy is invariant to rigid body transforms of a solid.
This invariance may be hard to learn exactly from data. We use a module $\mathcal{R}$ which applies \emph{Procrustes analysis}, i.e. finds and applies the rigid body transform which minimizes the Euclidean distance to a reference (we use the rest configuration). This is differentiable and closed-form.

\textbf{Encoding a linear elastic bias}.
The energy is approximated well by a linear elastic model when at rest:~${\tilde{E}^i(\tilde{u}_i) \approx \mathcal{R}(\rvu_i)^T A^i \mathcal{R}(\rvu_i)}$ for a stiffness matrix~$A^i$ depending on~$\xi_i$.
We scale our net's outputs by~$||\mathcal{R}(\rvu_i)||_2^2$ so that it needs only capture a ``scalar stiffness''~$\nicefrac{E}{||\mathcal{R}(\rvu_i)||_2^2}$ accounting for the geometry of~$A^i$ given~$\xi_i$ and for deviation from the linear elastic model.

\textbf{Parameterizing the log-stiffness}.
The energy of a component $\tilde{E}^i(u_{0, i})$ is nonnegative, and the ratio of energy to a linear elastic approximation varies over many orders of magnitude.
We parameterize the log of the scalar stiffness with our neural network $f_\phi$ rather than the stiffness.

\textbf{Log-stiffness loss}. We wish to find neural network parameters $\phi$ which lead to accurate energy predictions for many different orders of magnitude of energy and displacement.
Minimizing the~$\ell^2$ loss between predicted and true energies penalizes errors in predicting large energies more than proportional errors predicting small energies.
Instead, we take the~$\ell^2$ loss between the predicted log-stiffness~$f_\phi (\mathcal{R}(\rvu), \xi)$ and the effective ground-truth log-stiffness,~$\log\nicefrac{ \tilde{E}(\tilde{u})}{||\mathcal{R}(\rvu)||_2^2}$.

\textbf{Sobolev training with gradients and Hessian-vector products}.
"Sobolev training" on derivatives of a target function can aid generalization \citep{czarnecki2017sobolev}.
Accuracy of CES' derivatives is crucial, so we Sobolev train on energy gradients and Hessians.
We obtain ground-truth gradients cheaply via the adjoint method \citep{lions1971optimal}. Given a solution $u_i$ to the PDE in $\Omega_i$ with boundary conditions $\tilde{u}_i$, the gradient~${\nabla_{\tilde{u}_i} \tilde{E}_i(\tilde{u}_i)}$ requires solving a linear system with the same cost as one Newton step of solving the PDE \citep{mitusch2019dolfin}.
The spline is a linear map $\mathcal{M}$ from $\rvu_i$ to $\tilde{u}_i$ in the finite element basis, so~${\nabla_{\rvu_i} \tilde{E}_i(\tilde{u}_i) = \mathcal{M}^T \nabla_{\tilde{u}_i} \tilde{E}_i(\tilde{u}_i)}$. The surrogate gradient,~${\nabla_{\rvu_i} \hat{E}_\phi(\rvu_i, \xi_i)}$,
is computed with one backward pass. Given solution and gradient, we compute $\nabla^2_{\rvu}\tilde{E}$
with one linear solve per entry of $\rvu$. As $\rvu \in \mathbb{R}^{72}$ and many more than 72 Newton steps are usually needed to solve the PDE, this does not dominate the cost of data collection.
Computing the full Hessian of the surrogate energy,~${\nabla^2_{\rvu_i}\hat{E}_\phi(\rvu_i, \xi_i)}$, would require $2n$ backward passes.
Instead we train on Hessian-vector products, which require only one additional backward pass.

\textbf{Cosine distance loss for Sobolev training}.
Energy gradient and Hessian values vary over many orders of magnitude, with
higher energies leading to larger derivatives.
We wish our model to be accurate across a range of operating conditions.
Rather than placing an~$\ell^2$ loss on the gradient and Hessian-vector products as in \citet{czarnecki2017sobolev}, we minimize the cosine distance between ground truth and approximate gradients and Hessians, which is naturally bounded in $[0, 1]$.
%\vspace{-0.5cm}
